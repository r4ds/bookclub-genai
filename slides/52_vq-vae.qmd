---
engine: knitr
title: "52. VQ-VAE"
---

# Learning objectives

::: nonincremental
- Introduce VQ-VAE
:::

::: notes
- You can add notes on each slide with blocks like this!
- Load a deck in the browser and type "s" to see these notes.
:::

# Variational Auto-Encoders

## Variational Auto-Encoders

In AI architecture, **variational autoencoders** simulate the latent space (between the encoder and decoder)---usually with a mixture of Gaussian functions---to maximize the ELBO (**evidence lower bound**)	

![VAE Gaussians](images/VAE_gaussians.png)

* [image source](https://www.researchgate.net/publication/379478988_HGMVAE_hierarchical_disentanglement_in_Gaussian_mixture_variational_autoencoder)

## without

![without VAE](images/VAE_before.png)

* image source: [Jeff Orchard](https://www.youtube.com/watch?v=FSBLj74Qy4I)

## with

![with VAE](images/VAE_after.png)

* image source: [Jeff Orchard](https://www.youtube.com/watch?v=FSBLj74Qy4I)

# VQ-VAE

## VQ-VAE

**Vector quantized variational autoencoders** (VQ-VAE) utilize an *discrete embedding space*

* for example: 32x32 embedding space of vectors

![VQ-VAE](images/VQ_VAE.png)

* [image source](https://www.researchgate.net/publication/370101880_DL-based_Generation_of_facial_portraits_from_diverse_data_sources)
