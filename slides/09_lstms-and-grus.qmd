---
engine: knitr
title: "9. LSTMs and GRUs"
format: revealjs
---


# LSTMs
From: [Understanding LSTM Networks](https://colah.github.io/posts/2015-08-Understanding-LSTMs/)

## Long Short-Term Memory nets {style="font-size:80%"}

- A special kind of RNNs capable of learning long-term dependencies (no vanishing gradient)

- Invented by Hochreiter and Schmidhuber in 1995. LSTMs became the default choice for RNN architecture.

If we would like to predict the words in orange: 

> Alice is allergic to nuts. (...). She canâ€™t eat `peanut butter`.

If there were many sentences in (...), the context might get lost in a standard RNN.

## LSTMs structure

:::{style="font-size:90%"}
A chain-like structure just like RNNs.

4 modules instead of a single hidden layer: the cell state, the forget gate layer, the input gate layer, and the output gate.

:::
![LSTM structure](images/LSTM3-chain.png){.nostretch width="7
0%"}


## Sigmoid and tanh

![Sigmoid and tanh](images/sigmoid_tan.png){.nostretch width="40%"}

::::{.columns}
:::{.column width="50%"}
$$\sigma(z)=\frac{1}{1+e^{-z}}$$
:::
:::{.column width="50%"}
$$\text{tanh}(z)=\frac{e^z-e^{-z}}{e^z+e^{-z}}$$
:::
::::

## The cell state

The cell state represents the Long-Term Memory

It passes vectors without weights.

LSTM can remove or add information to the cell state

![LSTM cell state](images/LSTM3-C-line.png)

## The forget gate layer

![LSTM forget gate](images/LSTM3-focus-f.png)

To decide what information we're keeping from the **previous output**. It uses a *sigmoid* function:

  - 0 completely forget this
  - 1 completely keep this. 

## Input gate layer

It decides which values we'll update:

- The `tanh` layer creates a vector of new candidate values, a **potential memory** to add to the long-term memory
- The [$\sigma$]{style="color:#ffa07a"} layer decides the percentage of the potential memory to be added to the state.

![LSTM input gate](images/LSTM3-focus-i.png)

## Update the cell state

- We forget the things we decided to forget and add the candidate values, scaled by how much we decided to update each state value.

![LSTM update cell state](images/LSTM3-focus-C.png)

## The output gate

It is based on our cell state, but with a filtered version to create a potential new short term memory

Again, a `tanh` and a [$\sigma$]{style="color:#ffa07a"} to decide what to update and by how much. 

![LSTM output gate](images/LSTM3-focus-o.png)

# GRUs

## Gated Recurrent Unit

:::{style='font-size:70%'}
- A variation of the LSTM with two gates: 

  - **reset gate**: how much of the previous state we want to remember
  - **update gate**: how much of the new state we'll copy to the old one

![GRUs](images/LSTM3-var-GRU.png){.nostretch width="60%"}

- Introduced in 2014, as a simplification of LSTMs, with fewer parameters. 

- No performance difference between SLTM and GRU
:::
